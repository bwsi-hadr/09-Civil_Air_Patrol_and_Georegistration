{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Civil Air Patrol and Georegistration\n",
    "\n",
    "In the previous session, we saw how a series of 2D images taken of a 3D scene can be used to recover the 3D information, by exploiting geometric constraints of the cameras. Now the question is, how do we take this technique and apply it in a disaster response scenario?\n",
    "\n",
    "We are going to look at a specific case study, using images from the Low Altitude Disaster Imagery (LADI) dataset, taken by the Civil Air Patrol (CAP). As we work with this dataset, keep in mind the two major questions from the previous lecture:\n",
    "\n",
    "- _What_ is in an image (e.g. debris, buildings, etc.)?\n",
    "- _Where_ are these things located _in 3D space_ ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Civil Air Patrol\n",
    "Civil Air Patrol (CAP) is the civilian auxiliary of the United States Air Force (USAF). The origins of CAP date back to the pre-World War II era. As the Axis powers became a growing threat to the world, civilian aviators in the United States feared that the government would shut down general aviation as a precautionary measure. These aviators thus had to prove to the federal government that civilian aviation was not only not a danger, but actually a benefit to the war effort. \n",
    "\n",
    "As a result of these efforts, two separate programs were created. One was a Civilian Pilot Training Program, intended to increase the available people that could operate an aircraft should the need to deploy additional troops arise. The second actually called for the organization of civilian aviators and opened the door to the creation of CAP. \n",
    "\n",
    "Once the United States entered WWII proper, CAP began to embark a plethora of activities, some of which are still practiced today. They continued to do cadet education programs. They also began patrolling the coasts and borders. Finally, they started in 1942 conducting search and rescue (SAR) missions. These missions were a resounding success, and one of the main components of CAP today.\n",
    "\n",
    "CAP has five congressionally mandated missions:\n",
    "\n",
    "(1) To provide an organization toâ€”\n",
    "(A) encourage and aid citizens of the United States in contributing their efforts, services, and resources in developing aviation and in maintaining air supremacy; and\n",
    "(B) encourage and develop by example the voluntary contribution of private citizens to the public welfare.\n",
    "\n",
    "(2) To provide aviation education and training especially to its senior and cadet members.\n",
    "\n",
    "(3) To encourage and foster civil aviation in local communities.\n",
    "\n",
    "(4) To provide an organization of private citizens with adequate facilities to assist in meeting local and national emergencies.\n",
    "\n",
    "(5) To assist the Department of the Air Force in fulfilling its noncombat programs and missions.\n",
    "\n",
    "source: https://www.law.cornell.edu/uscode/text/36/40302\n",
    "\n",
    "CAP's main series of missions revolve around emergency response. CAP is involved in roughly 85% of all SAR missions in the United States and its territories. After natural disasters, CAP is responsible for assessing damage in affected communities, delivering supplies, providing transportation, in addition to its usual SAR missions. \n",
    "\n",
    "https://kvia.com/health/2020/06/18/el-paso-civil-air-patrol-flying-virus-tests-to-labs-in-money-saving-effort-for-texas/\n",
    "\n",
    "https://www.southernminn.com/article_2c5739a5-826f-53bb-a658-922fb1aa1627.html\n",
    "\n",
    "Part of their emergency programming is taking aerial imagery of affected areas. This imagery is the highest resolution, most timely imagery that we have available of a post-disaster situation. Even the highest resolution satellite imagery is often either limited in their geographical coverage, not very timely or occluded by clouds. These are images taken of Puerto Rico after Hurricane Maria in 2017.\n",
    "\n",
    "<img src=\"notebook_images/A0008AP-_932ec345-75a9-4005-9879-da06ba0af37e.jpg\" width=\"500\"  />\n",
    "\n",
    "<img src=\"notebook_images/A0016-52_e71b5e09-ec3c-4ea6-8ac8-b9d1e4b714cb.jpg\" width=\"500\"  />\n",
    "\n",
    "<img src=\"notebook_images/A0016-54_f5273b60-dec4-4617-8f01-d67f16001dcb.jpg\" width=\"500\"  />\n",
    "\n",
    "CAP has taken hundreds of thousands of images of disaster-affected areas in the past decades. And yet, even though it is some of the best imagery we have access to, it is rarely if ever used in practice. _Why?_\n",
    "\n",
    "## The LADI dataset\n",
    "Part of the effort in making CAP imagery more useful is trying to make more sense of the content of the images. To that end, researchers at MIT Lincoln Laboratory released the Low Altitude Disaster Imagery (LADI) dataset. This dataset contains hundreds of thousands of CAP images that have crowdsourced labels corresponding to infrastructure, environment and damage categories. This begins to answer the first of the two questions we set out initially. We'll start working on these labels tomorrow. For now, we will solely focus on the images themselves.\n",
    "\n",
    "<img src=\"notebook_images/labels.png\" width=\"500\"  />\n",
    "\n",
    "What are some of the limitations of this dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Imagine you have acquired $200,000 to implement some improvement to the way CAP takes aerial imagery. Hurricane season starts in five months, so whatever improvements need to be implemented by then. Separate into your breakout rooms and answer the following questions:\n",
    "- What specific hurdles to using CAP images do you want to address? Identify at least two.\n",
    "- Design a proposal to address the challenges you identified above, taking into account the budget and time constraints. Improvements can be of any sort (technical, political, social, etc).\n",
    "- What are the advantages and disadvantages of implementing your proposal?\n",
    "- Identify at least three different stakeholder groups in this situation. What are their specific needs? How does your proposal address these needs? How does your proposal fall short?\n",
    "- Draw out a budget breakdown and a timeline, as well as a breakdown of which stakeholders you are prioritizing and why. Prepare to present these at 1:30pm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3D Reconstruction in a Real World Reference Frame\n",
    "\n",
    "If we want to answer the second of our two guiding questions, we must be able to make a translation between where something is in the image and its location in real world coordinates. Let's stake stock of what tools we have thus far. We spent a good amount of time discussing structure from motion as a way to reconstruct a 3D scene from 2D images. Recall the limitations of this approach:\n",
    "- There need to be more than one image in a sequence.\n",
    "- Sequential images need to have enough overlap that there are common features.\n",
    "- At least one pair of sequential images must have sufficient translation such that the problem is not ill-posed.\n",
    "- The reconstruction is given in an arbitrary reference frame up to scale. \n",
    "\n",
    "What does that last point mean? The arbitrary reference part refers to the fact that the origin and the axes are aligned with the first camera. The up to scale part means that all distances are preserved up to a factor $\\lambda$. Therefore the scene retains the general shape, but the size of the scene is not conserved. Without additional information, it is impossible to know how the reconstructed scene relates to any other reference frame, and translating the reconstruction to real world coordinates is impossible.\n",
    "\n",
    "However, recall that we do typically have at least a coarse estimate of the camera's GPS coordinates, therefore we have estimates of the distances between sequential cameras. Consider a reconstruction of just two images. Then a good estimate of $\\lambda$ is:\n",
    "\n",
    "$\\lambda = \\frac{D_{GPS}}{D_{reconstruction}}$\n",
    "\n",
    "This is slightly more complicated for more than two images. Typically, a solver will initialize the camera positions at their GPS coordinates and use bundle adjustment to correct the errors in the GPS measurements, although certainly there's more than one way to do this.\n",
    "\n",
    "Let's give this a shot and see what happens! As it so happens, OpenSfM is already equipped to handle GPS coordinates. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import open3d as o3d\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-07-14 20:43:07,033 INFO: Extracting EXIF for image_url_pr_10_13_sample_07.jpg\n",
      "2020-07-14 20:43:07,142 INFO: Extracting EXIF for image_url_pr_10_13_sample_12.jpg\n",
      "2020-07-14 20:43:07,246 INFO: Extracting EXIF for image_url_pr_10_13_sample_08.jpg\n",
      "2020-07-14 20:43:07,344 INFO: Extracting EXIF for image_url_pr_10_13_sample_11.jpg\n",
      "2020-07-14 20:43:07,440 INFO: Extracting EXIF for image_url_pr_10_13_sample_13.jpg\n",
      "2020-07-14 20:43:08,968 INFO: Extracting ROOT_HAHOG features for image image_url_pr_10_13_sample_07.jpg\n",
      "2020-07-14 20:43:10,470 DEBUG: Found 5212 points in 1.0020146369934082s\n",
      "2020-07-14 20:43:10,471 DEBUG: No segmentation for image_url_pr_10_13_sample_07.jpg, no features masked.\n",
      "2020-07-14 20:43:10,509 INFO: Extracting ROOT_HAHOG features for image image_url_pr_10_13_sample_12.jpg\n",
      "2020-07-14 20:43:12,027 DEBUG: Found 5230 points in 0.9973897933959961s\n",
      "2020-07-14 20:43:12,028 DEBUG: No segmentation for image_url_pr_10_13_sample_12.jpg, no features masked.\n",
      "2020-07-14 20:43:12,061 INFO: Extracting ROOT_HAHOG features for image image_url_pr_10_13_sample_08.jpg\n",
      "2020-07-14 20:43:13,561 DEBUG: Found 5176 points in 0.9943075180053711s\n",
      "2020-07-14 20:43:13,562 DEBUG: No segmentation for image_url_pr_10_13_sample_08.jpg, no features masked.\n",
      "2020-07-14 20:43:13,593 INFO: Extracting ROOT_HAHOG features for image image_url_pr_10_13_sample_11.jpg\n",
      "2020-07-14 20:43:15,074 DEBUG: Found 5229 points in 0.9770879745483398s\n",
      "2020-07-14 20:43:15,075 DEBUG: No segmentation for image_url_pr_10_13_sample_11.jpg, no features masked.\n",
      "2020-07-14 20:43:15,107 INFO: Extracting ROOT_HAHOG features for image image_url_pr_10_13_sample_13.jpg\n",
      "2020-07-14 20:43:16,609 DEBUG: Found 5258 points in 0.9994611740112305s\n",
      "2020-07-14 20:43:16,609 DEBUG: No segmentation for image_url_pr_10_13_sample_13.jpg, no features masked.\n",
      "2020-07-14 20:43:18,097 INFO: Matching 6 image pairs\n",
      "2020-07-14 20:43:18,107 INFO: Computing pair matching with 1 processes\n",
      "2020-07-14 20:43:18,138 DEBUG: No segmentation for image_url_pr_10_13_sample_11.jpg, no features masked.\n",
      "2020-07-14 20:43:18,181 DEBUG: No segmentation for image_url_pr_10_13_sample_07.jpg, no features masked.\n",
      "2020-07-14 20:43:18,737 DEBUG: Matching image_url_pr_10_13_sample_11.jpg and image_url_pr_10_13_sample_07.jpg.  Matcher: FLANN (symmetric) T-desc: 0.599 Matches: FAILED\n",
      "2020-07-14 20:43:18,747 DEBUG: No segmentation for image_url_pr_10_13_sample_08.jpg, no features masked.\n",
      "2020-07-14 20:43:19,063 DEBUG: Matching image_url_pr_10_13_sample_11.jpg and image_url_pr_10_13_sample_08.jpg.  Matcher: FLANN (symmetric) T-desc: 0.317 Matches: FAILED\n",
      "2020-07-14 20:43:19,072 DEBUG: No segmentation for image_url_pr_10_13_sample_12.jpg, no features masked.\n",
      "2020-07-14 20:43:19,411 DEBUG: Matching image_url_pr_10_13_sample_11.jpg and image_url_pr_10_13_sample_12.jpg.  Matcher: FLANN (symmetric) T-desc: 0.334 T-robust: 0.004 T-total: 0.338 Matches: 1754 Robust: 1744 Success: True\n",
      "2020-07-14 20:43:19,421 DEBUG: No segmentation for image_url_pr_10_13_sample_13.jpg, no features masked.\n",
      "2020-07-14 20:43:19,782 DEBUG: Matching image_url_pr_10_13_sample_11.jpg and image_url_pr_10_13_sample_13.jpg.  Matcher: FLANN (symmetric) T-desc: 0.361 T-robust: 0.002 T-total: 0.362 Matches: 543 Robust: 493 Success: True\n",
      "2020-07-14 20:43:19,783 DEBUG: Image image_url_pr_10_13_sample_11.jpg matches: 2 out of 4\n",
      "2020-07-14 20:43:19,935 DEBUG: Matching image_url_pr_10_13_sample_08.jpg and image_url_pr_10_13_sample_07.jpg.  Matcher: FLANN (symmetric) T-desc: 0.149 T-robust: 0.002 T-total: 0.150 Matches: 772 Robust: 759 Success: True\n",
      "2020-07-14 20:43:19,935 DEBUG: Image image_url_pr_10_13_sample_08.jpg matches: 1 out of 1\n",
      "2020-07-14 20:43:20,100 DEBUG: Matching image_url_pr_10_13_sample_13.jpg and image_url_pr_10_13_sample_12.jpg.  Matcher: FLANN (symmetric) T-desc: 0.158 T-robust: 0.004 T-total: 0.162 Matches: 1696 Robust: 1679 Success: True\n",
      "2020-07-14 20:43:20,100 DEBUG: Image image_url_pr_10_13_sample_13.jpg matches: 1 out of 1\n",
      "2020-07-14 20:43:20,100 DEBUG: Image image_url_pr_10_13_sample_07.jpg matches: 0 out of 0\n",
      "2020-07-14 20:43:20,101 DEBUG: Image image_url_pr_10_13_sample_12.jpg matches: 0 out of 0\n",
      "2020-07-14 20:43:20,101 INFO: Matched 6 pairs for 5 ref_images (perspective-perspective: 6) in 2.003381006001291 seconds (0.3338969043331114 seconds/pair).\n",
      "2020-07-14 20:43:21,647 INFO: reading features\n",
      "2020-07-14 20:43:21,694 DEBUG: Merging features onto tracks\n",
      "2020-07-14 20:43:21,729 DEBUG: Good tracks: 3427\n",
      "2020-07-14 20:43:23,320 INFO: Starting incremental reconstruction\n",
      "2020-07-14 20:43:23,354 INFO: Starting reconstruction with image_url_pr_10_13_sample_12.jpg and image_url_pr_10_13_sample_13.jpg\n",
      "2020-07-14 20:43:23,504 INFO: Two-view reconstruction inliers: 1697 / 1697\n",
      "2020-07-14 20:43:23,722 INFO: Triangulated: 1519\n",
      "2020-07-14 20:43:23,743 DEBUG: Ceres Solver Report: Iterations: 2, Initial cost: 4.680116e+02, Final cost: 4.658281e+02, Termination: CONVERGENCE\n",
      "2020-07-14 20:43:23,871 DEBUG: Ceres Solver Report: Iterations: 2, Initial cost: 4.663767e+02, Final cost: 4.658091e+02, Termination: CONVERGENCE\n",
      "2020-07-14 20:43:24,156 DEBUG: Ceres Solver Report: Iterations: 13, Initial cost: 2.326037e+01, Final cost: 1.812864e+01, Termination: CONVERGENCE\n",
      "2020-07-14 20:43:24,163 INFO: Removed outliers: 0\n",
      "2020-07-14 20:43:24,163 INFO: -------------------------------------------------------\n",
      "2020-07-14 20:43:24,176 INFO: image_url_pr_10_13_sample_11.jpg resection inliers: 727 / 728\n",
      "2020-07-14 20:43:24,199 DEBUG: Ceres Solver Report: Iterations: 4, Initial cost: 7.153364e+01, Final cost: 5.037370e+01, Termination: CONVERGENCE\n",
      "2020-07-14 20:43:24,199 INFO: Adding image_url_pr_10_13_sample_11.jpg to the reconstruction\n",
      "2020-07-14 20:43:24,319 INFO: Re-triangulating\n",
      "2020-07-14 20:43:25,392 DEBUG: Ceres Solver Report: Iterations: 54, Initial cost: 9.439748e+01, Final cost: 5.471620e+01, Termination: CONVERGENCE\n",
      "2020-07-14 20:43:25,838 DEBUG: Ceres Solver Report: Iterations: 13, Initial cost: 5.745929e+01, Final cost: 5.677108e+01, Termination: CONVERGENCE\n",
      "2020-07-14 20:43:25,850 INFO: Removed outliers: 0\n",
      "2020-07-14 20:43:25,852 INFO: -------------------------------------------------------\n",
      "2020-07-14 20:43:25,855 INFO: Some images can not be added\n",
      "2020-07-14 20:43:25,855 INFO: -------------------------------------------------------\n",
      "2020-07-14 20:43:26,578 DEBUG: Ceres Solver Report: Iterations: 31, Initial cost: 6.548426e+01, Final cost: 5.756927e+01, Termination: CONVERGENCE\n",
      "2020-07-14 20:43:26,591 INFO: Removed outliers: 0\n",
      "2020-07-14 20:43:26,615 INFO: {'points_count': 2495, 'cameras_count': 3, 'observations_count': 5823, 'average_track_length': 2.333867735470942, 'average_track_length_notwo': 3.0}\n",
      "2020-07-14 20:43:26,615 INFO: Starting reconstruction with image_url_pr_10_13_sample_07.jpg and image_url_pr_10_13_sample_08.jpg\n",
      "2020-07-14 20:43:26,667 INFO: Two-view reconstruction inliers: 759 / 759\n",
      "2020-07-14 20:43:26,718 INFO: Triangulated: 759\n",
      "2020-07-14 20:43:26,731 DEBUG: Ceres Solver Report: Iterations: 2, Initial cost: 4.088774e+02, Final cost: 4.082088e+02, Termination: CONVERGENCE\n",
      "2020-07-14 20:43:26,892 DEBUG: Ceres Solver Report: Iterations: 2, Initial cost: 4.083684e+02, Final cost: 4.081465e+02, Termination: CONVERGENCE\n",
      "2020-07-14 20:43:27,104 DEBUG: Ceres Solver Report: Iterations: 16, Initial cost: 1.701690e+01, Final cost: 1.178096e+01, Termination: CONVERGENCE\n",
      "2020-07-14 20:43:27,108 INFO: Removed outliers: 0\n",
      "2020-07-14 20:43:27,108 INFO: -------------------------------------------------------\n",
      "2020-07-14 20:43:27,306 DEBUG: Ceres Solver Report: Iterations: 12, Initial cost: 1.836677e+01, Final cost: 1.178097e+01, Termination: CONVERGENCE\n",
      "2020-07-14 20:43:27,310 INFO: Removed outliers: 0\n",
      "2020-07-14 20:43:27,320 INFO: {'points_count': 759, 'cameras_count': 2, 'observations_count': 1518, 'average_track_length': 2.0, 'average_track_length_notwo': -1}\n",
      "2020-07-14 20:43:27,320 INFO: Reconstruction 0: 3 images, 2495 points\n",
      "2020-07-14 20:43:27,320 INFO: Reconstruction 1: 2 images, 759 points\n",
      "2020-07-14 20:43:27,320 INFO: 2 partial reconstructions in total.\n"
     ]
    }
   ],
   "source": [
    "# Take initial guess of intrinsic parameters through metadata\n",
    "!opensfm extract_metadata CAP_sample_1\n",
    "\n",
    "# Detect features points \n",
    "!opensfm detect_features CAP_sample_1\n",
    "\n",
    "# Match feature points across images\n",
    "!opensfm match_features CAP_sample_1\n",
    "\n",
    "# This creates \"tracks\" for the features. That is to say, if a feature in image 1 is matched with one in image 2,\n",
    "# and in turn that one is matched with one in image 3, then it links the matches between 1 and 3. \n",
    "!opensfm create_tracks CAP_sample_1\n",
    "\n",
    "# Calculates the essential matrix, the camera pose and the reconstructed feature points\n",
    "!opensfm reconstruct CAP_sample_1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding the --all command to include all partial reconstructions\n",
    "!opensfm export_ply --all CAP_sample_1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d213d91213c64deab988c554aa5db4cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "JVisualizer with 1 geometries"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import open3d as o3d\n",
    "from open3d import JVisualizer\n",
    "\n",
    "# it turns out that we have two partial reconstructions from the reconstruct command\n",
    "# open3d actually has a very convenient way of combining point clouds, just by using the + operator\n",
    "pcd = o3d.io.read_point_cloud(\"CAP_sample_1/reconstruction_files/reconstruction_0.ply\")\n",
    "pcd += o3d.io.read_point_cloud(\"CAP_sample_1/reconstruction_files/reconstruction_1.ply\")\n",
    "visualizer = JVisualizer()\n",
    "visualizer.add_geometry(pcd)\n",
    "visualizer.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what are we seeing? We see two collections of points, both mostly coplanar internally (which we expect, given that this is a mostly planar scene), but the two sets are not aligned with each other! Let's look a bit more closely..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# here, we're just going to plot the z (altitude) values of the reconstructed points\n",
    "point_coord = np.asarray(pcd.points)\n",
    "plt.hist(point_coord[:, 2].ravel())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So not only are the points misaligned, but we're getting wild altitude values! **What's going on?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "Let's make a critical assumption: all of the image coordinates (the GPS coordinates of the camera as it takes an image) all lie on a plane (in the mathematical sense). Answer the following questions:\n",
    "- How many points are needed to specify a (mathematical) plane?\n",
    "- In addition to the number of points, what other requirement do those points need?\n",
    "- Look at the visualization above. Do the camera points fulfill that requirement?\n",
    "- One way to resolve the ambiguity is to determine what direction is \"up\" (i.e. pointing away from the center of the Earth). Propose a solution to determine the up-vector. You can either assume the same setup that we currently have or propose new sensors/other setups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>CLICK HERE TO SEE THE PROPOSED SOLUTION</summary>\n",
    "    We're going to make a fair (but limited) assumption that the ground is mostly flat. It turns out we can fit a plane through the reconstructed ground points and find a direction perpendicular to the plane (called the plane normal). If the ground is flat, then the normal should be close enough to the up direction. Note that this assumption does not hold for an area with a lot of inclination. In practice, we would most likely augment this with a Digital Elevation Model (DEM)\n",
    "\n",
    "</details>\n",
    "\n",
    "<img src=\"notebook_images/plane_normal.png\" width=\"500\"  />\n",
    "\n",
    "To implement the proposed solution, we can go to the CAP_sample_1/config.yaml file and modify \"align_orientation_prior\" from \"horizontal\" to \"plane_based\". Afterwards, we run the previous commands as usual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: opensfm: command not found\n"
     ]
    }
   ],
   "source": [
    "# This creates \"tracks\" for the features. That is to say, if a feature in image 1 is matched with one in image 2,\n",
    "# and in turn that one is matched with one in image 3, then it links the matches between 1 and 3. \n",
    "!opensfm create_tracks CAP_sample_1\n",
    "\n",
    "# Calculates the essential matrix, the camera pose and the reconstructed feature points\n",
    "!opensfm reconstruct CAP_sample_1\n",
    "\n",
    "# adding the --all command to include all partial reconstructions\n",
    "!opensfm export_ply --all CAP_sample_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Georegistration\n",
    "\n",
    "The process of assigning GPS coordinates to individual pixels is called _georegistration_ or _georeferencing_. This requires us to perform a final transformation from pixel coordinates *per each image* to the 3D reconstructed coordinates. Before doing so, it is worthwhile talking a bit about what exactly our 3D coordinate system is. \n",
    "\n",
    "You might recall that not all coordinate referece systems lend themselves well to geometric transformations. Specifically, we want our 3D coordinate system to be Cartesian (i.e. three orthogonal, right-handed axes). OpenSfM performs its reconstructions in what is known as a *local tangent plane coordinate system* called *local east, north, up (ENU) coordinates*. The way this works is, you select an origin somewhere in the world (in our case, it is saved in the reference_lla.json file), and you align your axes such that the x-axis is parallel to latitudes and increasing Eastward, the y-axis is parallel to meridians and increasing Northward, and the z-axis is pointing away from the center of the Earth. The image below shows how this works:\n",
    "\n",
    "<img src=\"notebook_images/enu.png\" width=\"500\"  />\n",
    "\n",
    "In order to convert from ENU coordinates to geodetic coordinates (i.e. latitude, longitude, altitude), you need to know the origin. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Origin of our reconstruction, as given by the reference_lla.json (made from the reconstruction)\n",
    "with open(\"CAP_sample_1/reference_lla.json\", \"r\") as f:\n",
    "    reference_lla = json.load(f)\n",
    "    latitude=reference_lla[\"latitude\"]\n",
    "    longitude=reference_lla[\"longitude\"]\n",
    "    altitude=reference_lla[\"altitude\"]\n",
    "\n",
    "# This is the json file that contains the reconstructed feature points\n",
    "with open(\"CAP_sample_1/reconstruction.json\", \"r\") as f:\n",
    "    reconstructions = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a bit of work we need to go through to finalize the georegistration. First, we need to match the reconstructed features with the features on an image the tracks.csv file and the reconstruction.json can help us do that. The columns of tracks are as follows: image name, track ID (ID of the reconstructed point), feature ID (ID of the feature within the image), the *normalized* image coordinates x and y, the normalization factor s, and the color of the feature RGB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from opensfm.features import denormalized_image_coordinates\n",
    "\n",
    "# reading the csv\n",
    "tracks = pd.read_csv(\"CAP_sample_1/tracks.csv\", sep=\"\\t\", skiprows=1, names=[\"image_name\", \"track_id\", \"feature_id\", \"x\", \"y\", \"s\", \"R\", \"G\", \"B\"])\n",
    "\n",
    "# we need to denormalize the coordinates to turn them into regular pixel coordinates\n",
    "normalized_coor = tracks[[\"x\", \"y\", \"s\"]]\n",
    "denormalized_coor = denormalized_image_coordinates(normalized_coor.values, 4496, 3000)\n",
    "\n",
    "# create a new column with the denormalized coordinates\n",
    "tracks[\"denorm_x\"] = denormalized_coor[:, 0]\n",
    "tracks[\"denorm_y\"] = denormalized_coor[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to store the georegistration by creating a new .tif file for every CAP image. As you can recall, .tif files save not just the pixel data but also the projection that allows it to be displayed on top of other map data. There are two parts to doing this:\n",
    "- First, we need to create an _orthorectified_ image. Simply put, this is one that is transformed such that it looks as though you are looking at it from the top down. \n",
    "- Second, we need to add *ground control points* (GCPs) to the orthorectified image. GCPs are correspondences between world coordinates and pixel coordinates.\n",
    "\n",
    "Once we add the GCPs, any mapping software can plot the image such that the GCPs are aligned with their underlying coordinates. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import gdal, osr\n",
    "from pymap3d import enu2geodetic\n",
    "import random\n",
    "from skimage import transform\n",
    "\n",
    "if not os.path.isdir(\"CAP_sample_1/geotiff/\"):\n",
    "    os.mkdir(\"CAP_sample_1/geotiff/\")\n",
    "if not os.path.isdir(\"CAP_sample_1/ortho/\"):\n",
    "    os.mkdir(\"CAP_sample_1/ortho/\")\n",
    "\n",
    "for reconst in reconstructions:\n",
    "    for shot in reconst[\"shots\"]:\n",
    "        # some housekeeping\n",
    "        shot_name = shot.split(\".\")[0]\n",
    "        img = cv2.imread(\"CAP_sample_1/images/\"+shot)\n",
    "        shape = img.shape\n",
    "        \n",
    "        # here we get the features from the image and their corresponding reconstructed features\n",
    "        reconst_ids = list(map(int, reconst[\"points\"].keys()))\n",
    "        tracks_shot = tracks[(tracks[\"image_name\"] == shot) & (tracks[\"track_id\"].isin(reconst_ids))]\n",
    "        denorm_shot = np.round(tracks_shot[[\"denorm_x\", \"denorm_y\"]].values)\n",
    "        reconst_shot = np.array([reconst[\"points\"][str(point)][\"coordinates\"] for point in tracks_shot[\"track_id\"]])\n",
    "        \n",
    "        # we're going to create an image that is distorted to fit within the world coordinates\n",
    "        # pix_shot is just the reconstructed feature coordinates offset by some amount so that\n",
    "        # all coordinates are positive.\n",
    "        offset = np.min(reconst_shot[:, :2])\n",
    "        pix_shot = reconst_shot[:, :2]-np.multiply(offset, offset<0)\n",
    "        \n",
    "        # transformation for the new orthorectified image\n",
    "        H, inliers = cv2.findHomography(denorm_shot, pix_shot)\n",
    "        \n",
    "        # filtering out points that didn't fit the transformation\n",
    "        reconst_shot = reconst_shot[inliers.ravel()==1, :]\n",
    "        denorm_shot = np.round(denorm_shot[inliers.ravel()==1, :])\n",
    "        pix_shot = np.round(pix_shot[inliers.ravel()==1, :])\n",
    "        \n",
    "        # creating the ortho image\n",
    "        shape = tuple(np.max(pix_shot, axis=0).astype(int))\n",
    "        ortho_img = cv2.warpPerspective(img, H, shape)\n",
    "        cv2.imwrite(\"CAP_sample_1/ortho/\" + shot + \"_ortho.jpg\", ortho_img)\n",
    "        \n",
    "        # here we convert all of the reconstructed points into lat/lon coordinates\n",
    "        geo_shot = np.array([enu2geodetic(reconst_shot[i, 0],reconst_shot[i, 1],reconst_shot[i, 2],latitude,longitude,altitude) for i in range(reconst_shot.shape[0])])        \n",
    "        \n",
    "        idx = random.sample(range(len(geo_shot)), 10)\n",
    "        pix_shot_sample = pix_shot[idx, :]\n",
    "        geo_shot_sample = geo_shot[idx, :]\n",
    "                \n",
    "        # creating the Ground Control Points\n",
    "        orig_fn = \"CAP_sample_1/ortho/\" + shot + \"_ortho.jpg\"\n",
    "        fn = \"CAP_sample_1/geotiff/\" + shot_name + \"_GCP.tif\"\n",
    "        \n",
    "        orig_ds = gdal.Open(orig_fn)\n",
    "        gdal.GetDriverByName('GTiff').CreateCopy(fn, orig_ds)\n",
    "        ds = gdal.Open(fn, gdal.GA_Update)\n",
    "        sr = osr.SpatialReference()\n",
    "        sr.SetWellKnownGeogCS('WGS84')\n",
    "        \n",
    "        gcps = [gdal.GCP(geo_shot_sample[i, 1], geo_shot_sample[i, 0], 0, int(pix_shot_sample[i, 0]), int(pix_shot_sample[i, 1])) for i in range(geo_shot_sample.shape[0])]\n",
    "        \n",
    "        ds.SetGCPs(gcps, sr.ExportToWkt())\n",
    "        \n",
    "        ds = None\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "In the lesson folder, there is a spreadsheet with CAP images and their coordinates taken on October 13th, 2017. \n",
    "- Use geopandas to visualize the coordinates of all the images, and overlay it with some basemap\n",
    "- Select an area of those images that looks interesting to you. Use SfM to reconstruct at least 10 images\n",
    "- For those 10 images, select at least one and go through the georegistration process. Does the georegistration process yield good alignment with the ground truth? If not, why do you think that is?\n",
    "\n",
    "I **strongly** encourage you to tackle this as a team! Feel free to divide the tasks up as you see fit. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
