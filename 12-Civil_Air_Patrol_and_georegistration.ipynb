{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Civil Air Patrol and Georegistration\n",
    "\n",
    "In the previous session, we saw how a series of 2D images taken of a 3D scene can be used to recover the 3D information, by exploiting geometric constraints of the cameras. Now the question is, how do we take this technique and apply it in a disaster response scenario?\n",
    "\n",
    "We are going to look at a specific case study, using images from the Low Altitude Disaster Imagery (LADI) dataset, taken by the Civil Air Patrol (CAP). As we work with this dataset, keep in mind the two major questions from the previous lecture:\n",
    "\n",
    "- _What_ is in an image (e.g. debris, buildings, etc.)?\n",
    "- _Where_ are these things located _in 3D space_ ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Civil Air Patrol\n",
    "Civil Air Patrol (CAP) is the civilian auxiliary of the United States Air Force (USAF). The origins of CAP date back to the pre-World War II era. As the Axis powers became a growing threat to the world, civilian aviators in the United States feared that the government would shut down general aviation as a precautionary measure. These aviators thus had to prove to the federal government that civilian aviation was not only not a danger, but actually a benefit to the war effort. \n",
    "\n",
    "As a result of these efforts, two separate programs were created. One was a Civilian Pilot Training Program, intended to increase the available people that could operate an aircraft should the need to deploy additional troops arise. The second actually called for the organization of civilian aviators and opened the door to the creation of CAP. \n",
    "\n",
    "Once the United States entered WWII proper, CAP began to embark a plethora of activities, some of which are still practiced today. They continued to do cadet education programs. They also began patrolling the coasts and borders. Finally, they started in 1942 conducting search and rescue (SAR) missions. These missions were a resounding success, and one of the main components of CAP today.\n",
    "\n",
    "CAP has five congressionally mandated missions:\n",
    "\n",
    "(1) To provide an organization toâ€”\n",
    "(A) encourage and aid citizens of the United States in contributing their efforts, services, and resources in developing aviation and in maintaining air supremacy; and\n",
    "(B) encourage and develop by example the voluntary contribution of private citizens to the public welfare.\n",
    "\n",
    "(2) To provide aviation education and training especially to its senior and cadet members.\n",
    "\n",
    "(3) To encourage and foster civil aviation in local communities.\n",
    "\n",
    "(4) To provide an organization of private citizens with adequate facilities to assist in meeting local and national emergencies.\n",
    "\n",
    "(5) To assist the Department of the Air Force in fulfilling its noncombat programs and missions.\n",
    "\n",
    "source: https://www.law.cornell.edu/uscode/text/36/40302\n",
    "\n",
    "CAP's main series of missions revolve around emergency response. CAP is involved in roughly 85% of all SAR missions in the United States and its territories. After natural disasters, CAP is responsible for assessing damage in affected communities, delivering supplies, providing transportation, in addition to its usual SAR missions. \n",
    "\n",
    "https://kvia.com/health/2020/06/18/el-paso-civil-air-patrol-flying-virus-tests-to-labs-in-money-saving-effort-for-texas/\n",
    "\n",
    "https://www.southernminn.com/article_2c5739a5-826f-53bb-a658-922fb1aa1627.html\n",
    "\n",
    "Part of their emergency programming is taking aerial imagery of affected areas. This imagery is the highest resolution, most timely imagery that we have available of a post-disaster situation. Even the highest resolution satellite imagery is often either limited in their geographical coverage, not very timely or occluded by clouds. These are images taken of Puerto Rico after Hurricane Maria in 2017.\n",
    "\n",
    "<img src=\"notebook_images/A0008AP-_932ec345-75a9-4005-9879-da06ba0af37e.jpg\" width=\"500\"  />\n",
    "\n",
    "<img src=\"notebook_images/A0016-52_e71b5e09-ec3c-4ea6-8ac8-b9d1e4b714cb.jpg\" width=\"500\"  />\n",
    "\n",
    "<img src=\"notebook_images/A0016-54_f5273b60-dec4-4617-8f01-d67f16001dcb.jpg\" width=\"500\"  />\n",
    "\n",
    "CAP has taken hundreds of thousands of images of disaster-affected areas in the past decades. And yet, even though it is some of the best imagery we have access to, it is rarely if ever used in practice. _Why?_\n",
    "\n",
    "## The LADI dataset\n",
    "Part of the effort in making CAP imagery more useful is trying to make more sense of the content of the images. To that end, researchers at MIT Lincoln Laboratory released the Low Altitude Disaster Imagery (LADI) dataset. This dataset contains hundreds of thousands of CAP images that have crowdsourced labels corresponding to infrastructure, environment and damage categories. This begins to answer the first of the two questions we set out initially. We'll start working on these labels tomorrow. For now, we will solely focus on the images themselves.\n",
    "\n",
    "<img src=\"notebook_images/labels.png\" width=\"500\"  />\n",
    "\n",
    "What are some of the limitations of this dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Imagine you have acquired $200,000 to implement some improvement to the way CAP takes aerial imagery. Hurricane season starts in five months, so whatever improvements need to be implemented by then. Separate into your breakout rooms and answer the following questions:\n",
    "- What specific hurdles to using CAP images do you want to address? Identify at least two.\n",
    "- Design a proposal to address the challenges you identified above, taking into account the budget and time constraints. Improvements can be of any sort (technical, political, social, etc).\n",
    "- What are the advantages and disadvantages of implementing your proposal?\n",
    "- Identify at least three different stakeholder groups in this situation. What are their specific needs? How does your proposal address these needs? How does your proposal fall short?\n",
    "- Draw out a budget breakdown and a timeline, as well as a breakdown of which stakeholders you are prioritizing and why. Prepare to present these at 1:30pm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3D Reconstruction in a Real World Reference Frame\n",
    "\n",
    "If we want to answer the second of our two guiding questions, we must be able to make a translation between where something is in the image and its location in real world coordinates. Let's stake stock of what tools we have thus far. We spent a good amount of time discussing structure from motion as a way to reconstruct a 3D scene from 2D images. Recall the limitations of this approach:\n",
    "- There need to be more than one image in a sequence.\n",
    "- Sequential images need to have enough overlap that there are common features.\n",
    "- At least one pair of sequential images must have sufficient translation such that the problem is not ill-posed.\n",
    "- The reconstruction is given in an arbitrary reference frame up to scale. \n",
    "\n",
    "What does that last point mean? The arbitrary reference part refers to the fact that the origin and the axes are aligned with the first camera. The up to scale part means that all distances are preserved up to a factor $\\lambda$. Therefore the scene retains the general shape, but the size of the scene is not conserved. Without additional information, it is impossible to know how the reconstructed scene relates to any other reference frame, and translating the reconstruction to real world coordinates is impossible.\n",
    "\n",
    "However, recall that we do typically have at least a coarse estimate of the camera's GPS coordinates, therefore we have estimates of the distances between sequential cameras. Consider a reconstruction of just two images. Then a good estimate of $\\lambda$ is:\n",
    "\n",
    "$\\lambda = \\frac{D_{GPS}}{D_{reconstruction}}$\n",
    "\n",
    "This is slightly more complicated for more than two images. Typically, a solver will initialize the camera positions at their GPS coordinates and use bundle adjustment to correct the errors in the GPS measurements, although certainly there's more than one way to do this.\n",
    "\n",
    "Let's give this a shot and see what happens! As it so happens, OpenSfM is already equipped to handle GPS coordinates. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import open3d as o3d\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-07-16 18:33:28,600 INFO: Loading existing EXIF for image_url_pr_10_13_sample_12.jpg\n",
      "2020-07-16 18:33:28,600 INFO: Loading existing EXIF for image_url_pr_10_13_sample_11.jpg\n",
      "2020-07-16 18:33:28,600 INFO: Loading existing EXIF for image_url_pr_10_13_sample_08.jpg\n",
      "2020-07-16 18:33:28,600 INFO: Loading existing EXIF for image_url_pr_10_13_sample_07.jpg\n",
      "2020-07-16 18:33:28,601 INFO: Loading existing EXIF for image_url_pr_10_13_sample_13.jpg\n",
      "2020-07-16 18:33:30,558 INFO: Skip recomputing ROOT_HAHOG features for image image_url_pr_10_13_sample_12.jpg\n",
      "2020-07-16 18:33:30,558 INFO: Skip recomputing ROOT_HAHOG features for image image_url_pr_10_13_sample_11.jpg\n",
      "2020-07-16 18:33:30,558 INFO: Skip recomputing ROOT_HAHOG features for image image_url_pr_10_13_sample_08.jpg\n",
      "2020-07-16 18:33:30,558 INFO: Skip recomputing ROOT_HAHOG features for image image_url_pr_10_13_sample_07.jpg\n",
      "2020-07-16 18:33:30,558 INFO: Skip recomputing ROOT_HAHOG features for image image_url_pr_10_13_sample_13.jpg\n",
      "2020-07-16 18:33:32,495 INFO: Matching 6 image pairs\n",
      "2020-07-16 18:33:32,502 INFO: Computing pair matching with 1 processes\n",
      "2020-07-16 18:33:32,528 DEBUG: No segmentation for image_url_pr_10_13_sample_11.jpg, no features masked.\n",
      "2020-07-16 18:33:32,529 DEBUG: No segmentation for image_url_pr_10_13_sample_08.jpg, no features masked.\n",
      "2020-07-16 18:33:33,116 DEBUG: Matching image_url_pr_10_13_sample_11.jpg and image_url_pr_10_13_sample_08.jpg.  Matcher: FLANN (symmetric) T-desc: 0.589 Matches: FAILED\n",
      "2020-07-16 18:33:33,128 DEBUG: No segmentation for image_url_pr_10_13_sample_07.jpg, no features masked.\n",
      "2020-07-16 18:33:33,506 DEBUG: Matching image_url_pr_10_13_sample_11.jpg and image_url_pr_10_13_sample_07.jpg.  Matcher: FLANN (symmetric) T-desc: 0.380 Matches: FAILED\n",
      "2020-07-16 18:33:33,507 DEBUG: Image image_url_pr_10_13_sample_11.jpg matches: 0 out of 2\n",
      "2020-07-16 18:33:33,526 DEBUG: No segmentation for image_url_pr_10_13_sample_13.jpg, no features masked.\n",
      "2020-07-16 18:33:33,527 DEBUG: No segmentation for image_url_pr_10_13_sample_12.jpg, no features masked.\n",
      "2020-07-16 18:33:34,119 DEBUG: Matching image_url_pr_10_13_sample_13.jpg and image_url_pr_10_13_sample_12.jpg.  Matcher: FLANN (symmetric) T-desc: 0.587 T-robust: 0.004 T-total: 0.591 Matches: 1672 Robust: 1657 Success: True\n",
      "2020-07-16 18:33:34,302 DEBUG: Matching image_url_pr_10_13_sample_13.jpg and image_url_pr_10_13_sample_11.jpg.  Matcher: FLANN (symmetric) T-desc: 0.180 T-robust: 0.001 T-total: 0.181 Matches: 543 Robust: 511 Success: True\n",
      "2020-07-16 18:33:34,302 DEBUG: Image image_url_pr_10_13_sample_13.jpg matches: 2 out of 2\n",
      "2020-07-16 18:33:34,486 DEBUG: Matching image_url_pr_10_13_sample_08.jpg and image_url_pr_10_13_sample_07.jpg.  Matcher: FLANN (symmetric) T-desc: 0.181 T-robust: 0.002 T-total: 0.183 Matches: 767 Robust: 740 Success: True\n",
      "2020-07-16 18:33:34,486 DEBUG: Image image_url_pr_10_13_sample_08.jpg matches: 1 out of 1\n",
      "2020-07-16 18:33:34,679 DEBUG: Matching image_url_pr_10_13_sample_12.jpg and image_url_pr_10_13_sample_11.jpg.  Matcher: FLANN (symmetric) T-desc: 0.185 T-robust: 0.004 T-total: 0.190 Matches: 1748 Robust: 1742 Success: True\n",
      "2020-07-16 18:33:34,679 DEBUG: Image image_url_pr_10_13_sample_12.jpg matches: 1 out of 1\n",
      "2020-07-16 18:33:34,679 DEBUG: Image image_url_pr_10_13_sample_07.jpg matches: 0 out of 0\n",
      "2020-07-16 18:33:34,680 INFO: Matched 6 pairs for 5 ref_images (perspective-perspective: 6) in 2.1846528890000627 seconds (0.364108884833513 seconds/pair).\n",
      "2020-07-16 18:33:36,718 INFO: reading features\n",
      "2020-07-16 18:33:36,772 DEBUG: Merging features onto tracks\n",
      "2020-07-16 18:33:36,813 DEBUG: Good tracks: 3409\n",
      "2020-07-16 18:33:38,863 INFO: Starting incremental reconstruction\n",
      "2020-07-16 18:33:38,903 INFO: Starting reconstruction with image_url_pr_10_13_sample_11.jpg and image_url_pr_10_13_sample_12.jpg\n",
      "2020-07-16 18:33:38,985 INFO: Two-view reconstruction inliers: 1751 / 1757\n",
      "2020-07-16 18:33:39,139 INFO: Triangulated: 1295\n",
      "2020-07-16 18:33:39,184 DEBUG: Ceres Solver Report: Iterations: 3, Initial cost: 3.422112e+02, Final cost: 3.375927e+02, Termination: CONVERGENCE\n",
      "2020-07-16 18:33:39,318 DEBUG: Ceres Solver Report: Iterations: 2, Initial cost: 3.387361e+02, Final cost: 3.375565e+02, Termination: CONVERGENCE\n",
      "Align plane:  [ 3.39040403e-02 -9.99409497e-01  5.58338857e-03  1.60210664e-17]\n",
      "2020-07-16 18:33:39,644 DEBUG: Ceres Solver Report: Iterations: 40, Initial cost: 2.111931e+01, Final cost: 1.450936e+01, Termination: CONVERGENCE\n",
      "2020-07-16 18:33:39,650 INFO: Removed outliers: 0\n",
      "2020-07-16 18:33:39,652 INFO: -------------------------------------------------------\n",
      "2020-07-16 18:33:39,669 INFO: image_url_pr_10_13_sample_13.jpg resection inliers: 611 / 613\n",
      "2020-07-16 18:33:39,692 DEBUG: Ceres Solver Report: Iterations: 4, Initial cost: 7.801686e+01, Final cost: 4.691771e+01, Termination: CONVERGENCE\n",
      "2020-07-16 18:33:39,693 INFO: Adding image_url_pr_10_13_sample_13.jpg to the reconstruction\n",
      "2020-07-16 18:33:39,823 INFO: Re-triangulating\n",
      "Align plane:  [-1.49259630e-01  1.16891409e-01  9.81864533e-01  4.15007687e-15]\n",
      "2020-07-16 18:33:40,950 DEBUG: Ceres Solver Report: Iterations: 82, Initial cost: 9.673135e+01, Final cost: 4.933581e+01, Termination: CONVERGENCE\n",
      "2020-07-16 18:33:41,622 DEBUG: Ceres Solver Report: Iterations: 31, Initial cost: 5.985284e+01, Final cost: 5.692503e+01, Termination: CONVERGENCE\n",
      "2020-07-16 18:33:41,636 INFO: Removed outliers: 0\n",
      "2020-07-16 18:33:41,638 INFO: -------------------------------------------------------\n",
      "2020-07-16 18:33:41,641 INFO: Some images can not be added\n",
      "2020-07-16 18:33:41,641 INFO: -------------------------------------------------------\n",
      "Align plane:  [-1.38431379e-01  9.51735289e-02  9.85788391e-01  4.25937815e-14]\n",
      "2020-07-16 18:33:42,248 DEBUG: Ceres Solver Report: Iterations: 30, Initial cost: 6.481103e+01, Final cost: 5.692499e+01, Termination: CONVERGENCE\n",
      "2020-07-16 18:33:42,262 INFO: Removed outliers: 0\n",
      "2020-07-16 18:33:42,290 INFO: {'points_count': 2408, 'cameras_count': 3, 'observations_count': 5626, 'average_track_length': 2.336378737541528, 'average_track_length_notwo': 3.0}\n",
      "2020-07-16 18:33:42,290 INFO: Starting reconstruction with image_url_pr_10_13_sample_07.jpg and image_url_pr_10_13_sample_08.jpg\n",
      "2020-07-16 18:33:42,366 INFO: Two-view reconstruction inliers: 740 / 740\n",
      "2020-07-16 18:33:42,520 INFO: Triangulated: 740\n",
      "2020-07-16 18:33:42,531 DEBUG: Ceres Solver Report: Iterations: 2, Initial cost: 4.082157e+02, Final cost: 4.078742e+02, Termination: CONVERGENCE\n",
      "2020-07-16 18:33:42,596 DEBUG: Ceres Solver Report: Iterations: 2, Initial cost: 4.079646e+02, Final cost: 4.078735e+02, Termination: CONVERGENCE\n",
      "Align plane:  [ 0.01404673 -0.99989964 -0.00184198  0.        ]\n",
      "2020-07-16 18:33:42,711 DEBUG: Ceres Solver Report: Iterations: 16, Initial cost: 1.682259e+01, Final cost: 1.260410e+01, Termination: CONVERGENCE\n",
      "2020-07-16 18:33:42,716 INFO: Removed outliers: 0\n",
      "2020-07-16 18:33:42,717 INFO: -------------------------------------------------------\n",
      "Align plane:  [-4.97543791e-02  3.22788790e-02  9.98239739e-01 -4.92257619e-15]\n",
      "2020-07-16 18:33:42,881 DEBUG: Ceres Solver Report: Iterations: 12, Initial cost: 1.919790e+01, Final cost: 1.260410e+01, Termination: CONVERGENCE\n",
      "2020-07-16 18:33:42,885 INFO: Removed outliers: 0\n",
      "2020-07-16 18:33:42,893 INFO: {'points_count': 740, 'cameras_count': 2, 'observations_count': 1480, 'average_track_length': 2.0, 'average_track_length_notwo': -1}\n",
      "2020-07-16 18:33:42,893 INFO: Reconstruction 0: 3 images, 2408 points\n",
      "2020-07-16 18:33:42,893 INFO: Reconstruction 1: 2 images, 740 points\n",
      "2020-07-16 18:33:42,893 INFO: 2 partial reconstructions in total.\n"
     ]
    }
   ],
   "source": [
    "# Take initial guess of intrinsic parameters through metadata\n",
    "!opensfm extract_metadata CAP_sample_1\n",
    "\n",
    "# Detect features points \n",
    "!opensfm detect_features CAP_sample_1\n",
    "\n",
    "# Match feature points across images\n",
    "!opensfm match_features CAP_sample_1\n",
    "\n",
    "# This creates \"tracks\" for the features. That is to say, if a feature in image 1 is matched with one in image 2,\n",
    "# and in turn that one is matched with one in image 3, then it links the matches between 1 and 3. \n",
    "!opensfm create_tracks CAP_sample_1\n",
    "\n",
    "# Calculates the essential matrix, the camera pose and the reconstructed feature points\n",
    "!opensfm reconstruct CAP_sample_1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding the --all command to include all partial reconstructions\n",
    "!opensfm export_ply --all CAP_sample_1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8326f51db3c24c80b77f65537b81cfcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "JVisualizer with 1 geometries"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import open3d as o3d\n",
    "from open3d import JVisualizer\n",
    "\n",
    "# it turns out that we have two partial reconstructions from the reconstruct command\n",
    "# open3d actually has a very convenient way of combining point clouds, just by using the + operator\n",
    "pcd = o3d.io.read_point_cloud(\"CAP_sample_1/reconstruction_files/reconstruction_0.ply\")\n",
    "pcd += o3d.io.read_point_cloud(\"CAP_sample_1/reconstruction_files/reconstruction_1.ply\")\n",
    "visualizer = JVisualizer()\n",
    "visualizer.add_geometry(pcd)\n",
    "visualizer.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what are we seeing? We see two collections of points, both mostly coplanar internally (which we expect, given that this is a mostly planar scene), but the two sets are not aligned with each other! Let's look a bit more closely..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# here, we're just going to plot the z (altitude) values of the reconstructed points\n",
    "point_coord = np.asarray(pcd.points)\n",
    "plt.hist(point_coord[:, 2].ravel())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So not only are the points misaligned, but we're getting wild altitude values! **What's going on?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "Let's make a critical assumption: all of the image coordinates (the GPS coordinates of the camera as it takes an image) all lie on a plane (in the mathematical sense). Answer the following questions:\n",
    "- How many points are needed to specify a (mathematical) plane?\n",
    "- In addition to the number of points, what other requirement do those points need?\n",
    "- Look at the visualization above. Do the camera points fulfill that requirement?\n",
    "- One way to resolve the ambiguity is to determine what direction is \"up\" (i.e. pointing away from the center of the Earth). Propose a solution to determine the up-vector. You can either assume the same setup that we currently have or propose new sensors/other setups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>CLICK HERE TO SEE THE PROPOSED SOLUTION</summary>\n",
    "    We're going to make a fair (but limited) assumption that the ground is mostly flat. It turns out we can fit a plane through the reconstructed ground points and find a direction perpendicular to the plane (called the plane normal). If the ground is flat, then the normal should be close enough to the up direction. Note that this assumption does not hold for an area with a lot of inclination. In practice, we would most likely augment this with a Digital Elevation Model (DEM)\n",
    "\n",
    "</details>\n",
    "\n",
    "<img src=\"notebook_images/plane_normal.png\" width=\"500\"  />\n",
    "\n",
    "To implement the proposed solution, we can go to the CAP_sample_1/config.yaml file and modify \"align_orientation_prior\" from \"horizontal\" to \"plane_based\". Afterwards, we run the previous commands as usual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-07-16 18:33:47,250 INFO: reading features\n",
      "2020-07-16 18:33:47,305 DEBUG: Merging features onto tracks\n",
      "2020-07-16 18:33:47,347 DEBUG: Good tracks: 3409\n",
      "2020-07-16 18:33:49,447 INFO: Starting incremental reconstruction\n",
      "2020-07-16 18:33:49,488 INFO: Starting reconstruction with image_url_pr_10_13_sample_11.jpg and image_url_pr_10_13_sample_12.jpg\n",
      "2020-07-16 18:33:49,564 INFO: Two-view reconstruction inliers: 1751 / 1757\n",
      "2020-07-16 18:33:49,701 INFO: Triangulated: 1295\n",
      "2020-07-16 18:33:49,720 DEBUG: Ceres Solver Report: Iterations: 3, Initial cost: 3.422112e+02, Final cost: 3.375927e+02, Termination: CONVERGENCE\n",
      "2020-07-16 18:33:49,853 DEBUG: Ceres Solver Report: Iterations: 2, Initial cost: 3.387361e+02, Final cost: 3.375565e+02, Termination: CONVERGENCE\n",
      "Align plane:  [ 3.39040403e-02 -9.99409497e-01  5.58338857e-03  1.60210664e-17]\n",
      "2020-07-16 18:33:50,176 DEBUG: Ceres Solver Report: Iterations: 40, Initial cost: 2.111931e+01, Final cost: 1.450936e+01, Termination: CONVERGENCE\n",
      "2020-07-16 18:33:50,184 INFO: Removed outliers: 0\n",
      "2020-07-16 18:33:50,186 INFO: -------------------------------------------------------\n",
      "2020-07-16 18:33:50,202 INFO: image_url_pr_10_13_sample_13.jpg resection inliers: 611 / 613\n",
      "2020-07-16 18:33:50,226 DEBUG: Ceres Solver Report: Iterations: 4, Initial cost: 7.801686e+01, Final cost: 4.691771e+01, Termination: CONVERGENCE\n",
      "2020-07-16 18:33:50,226 INFO: Adding image_url_pr_10_13_sample_13.jpg to the reconstruction\n",
      "2020-07-16 18:33:50,396 INFO: Re-triangulating\n",
      "Align plane:  [-1.49259630e-01  1.16891409e-01  9.81864533e-01  1.43822959e-14]\n",
      "2020-07-16 18:33:51,484 DEBUG: Ceres Solver Report: Iterations: 82, Initial cost: 9.673135e+01, Final cost: 4.933581e+01, Termination: CONVERGENCE\n",
      "2020-07-16 18:33:52,153 DEBUG: Ceres Solver Report: Iterations: 31, Initial cost: 5.985284e+01, Final cost: 5.692503e+01, Termination: CONVERGENCE\n",
      "2020-07-16 18:33:52,167 INFO: Removed outliers: 0\n",
      "2020-07-16 18:33:52,168 INFO: -------------------------------------------------------\n",
      "2020-07-16 18:33:52,171 INFO: Some images can not be added\n",
      "2020-07-16 18:33:52,172 INFO: -------------------------------------------------------\n",
      "Align plane:  [-1.38431194e-01  9.51738076e-02  9.85788391e-01  4.00937495e-14]\n",
      "2020-07-16 18:33:52,802 DEBUG: Ceres Solver Report: Iterations: 30, Initial cost: 6.481103e+01, Final cost: 5.692499e+01, Termination: CONVERGENCE\n",
      "2020-07-16 18:33:52,816 INFO: Removed outliers: 0\n",
      "2020-07-16 18:33:52,845 INFO: {'points_count': 2408, 'cameras_count': 3, 'observations_count': 5626, 'average_track_length': 2.336378737541528, 'average_track_length_notwo': 3.0}\n",
      "2020-07-16 18:33:52,845 INFO: Starting reconstruction with image_url_pr_10_13_sample_07.jpg and image_url_pr_10_13_sample_08.jpg\n",
      "2020-07-16 18:33:52,923 INFO: Two-view reconstruction inliers: 740 / 740\n",
      "2020-07-16 18:33:53,083 INFO: Triangulated: 740\n",
      "2020-07-16 18:33:53,093 DEBUG: Ceres Solver Report: Iterations: 2, Initial cost: 4.082157e+02, Final cost: 4.078742e+02, Termination: CONVERGENCE\n",
      "2020-07-16 18:33:53,152 DEBUG: Ceres Solver Report: Iterations: 2, Initial cost: 4.079646e+02, Final cost: 4.078735e+02, Termination: CONVERGENCE\n",
      "Align plane:  [ 0.01404673 -0.99989964 -0.00184198  0.        ]\n",
      "2020-07-16 18:33:53,277 DEBUG: Ceres Solver Report: Iterations: 16, Initial cost: 1.682259e+01, Final cost: 1.260410e+01, Termination: CONVERGENCE\n",
      "2020-07-16 18:33:53,281 INFO: Removed outliers: 0\n",
      "2020-07-16 18:33:53,281 INFO: -------------------------------------------------------\n",
      "Align plane:  [-4.97543791e-02  3.22788790e-02  9.98239739e-01 -3.27998480e-14]\n",
      "2020-07-16 18:33:53,405 DEBUG: Ceres Solver Report: Iterations: 12, Initial cost: 1.919790e+01, Final cost: 1.260410e+01, Termination: CONVERGENCE\n",
      "2020-07-16 18:33:53,409 INFO: Removed outliers: 0\n",
      "2020-07-16 18:33:53,418 INFO: {'points_count': 740, 'cameras_count': 2, 'observations_count': 1480, 'average_track_length': 2.0, 'average_track_length_notwo': -1}\n",
      "2020-07-16 18:33:53,418 INFO: Reconstruction 0: 3 images, 2408 points\n",
      "2020-07-16 18:33:53,418 INFO: Reconstruction 1: 2 images, 740 points\n",
      "2020-07-16 18:33:53,418 INFO: 2 partial reconstructions in total.\n"
     ]
    }
   ],
   "source": [
    "# This creates \"tracks\" for the features. That is to say, if a feature in image 1 is matched with one in image 2,\n",
    "# and in turn that one is matched with one in image 3, then it links the matches between 1 and 3. \n",
    "!opensfm create_tracks CAP_sample_1\n",
    "\n",
    "# Calculates the essential matrix, the camera pose and the reconstructed feature points\n",
    "!opensfm reconstruct CAP_sample_1\n",
    "\n",
    "# adding the --all command to include all partial reconstructions\n",
    "!opensfm export_ply --all CAP_sample_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Georegistration\n",
    "\n",
    "The process of assigning GPS coordinates to individual pixels is called _georegistration_ or _georeferencing_. This requires us to perform a final transformation from pixel coordinates *per each image* to the 3D reconstructed coordinates. Before doing so, it is worthwhile talking a bit about what exactly our 3D coordinate system is. \n",
    "\n",
    "You might recall that not all coordinate referece systems lend themselves well to geometric transformations. Specifically, we want our 3D coordinate system to be Cartesian (i.e. three orthogonal, right-handed axes). OpenSfM performs its reconstructions in what is known as a *local tangent plane coordinate system* called *local east, north, up (ENU) coordinates*. The way this works is, you select an origin somewhere in the world (in our case, it is saved in the reference_lla.json file), and you align your axes such that the x-axis is parallel to latitudes and increasing Eastward, the y-axis is parallel to meridians and increasing Northward, and the z-axis is pointing away from the center of the Earth. The image below shows how this works:\n",
    "\n",
    "<img src=\"notebook_images/enu.png\" width=\"500\"  />\n",
    "\n",
    "In order to convert from ENU coordinates to geodetic coordinates (i.e. latitude, longitude, altitude), you need to know the origin. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Origin of our reconstruction, as given by the reference_lla.json (made from the reconstruction)\n",
    "with open(\"CAP_sample_1/reference_lla.json\", \"r\") as f:\n",
    "    reference_lla = json.load(f)\n",
    "    latitude=reference_lla[\"latitude\"]\n",
    "    longitude=reference_lla[\"longitude\"]\n",
    "    altitude=reference_lla[\"altitude\"]\n",
    "\n",
    "# This is the json file that contains the reconstructed feature points\n",
    "with open(\"CAP_sample_1/reconstruction.json\", \"r\") as f:\n",
    "    reconstructions = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a bit of work we need to go through to finalize the georegistration. First, we need to match the reconstructed features with the features on an image the tracks.csv file and the reconstruction.json can help us do that. The columns of tracks are as follows: image name, track ID (ID of the reconstructed point), feature ID (ID of the feature within the image), the *normalized* image coordinates x and y, the normalization factor s, and the color of the feature RGB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from opensfm.features import denormalized_image_coordinates\n",
    "\n",
    "# reading the csv\n",
    "tracks = pd.read_csv(\"CAP_sample_1/tracks.csv\", sep=\"\\t\", skiprows=1, names=[\"image_name\", \"track_id\", \"feature_id\", \"x\", \"y\", \"s\", \"R\", \"G\", \"B\"])\n",
    "\n",
    "# we need to denormalize the coordinates to turn them into regular pixel coordinates\n",
    "normalized_coor = tracks[[\"x\", \"y\", \"s\"]]\n",
    "denormalized_coor = denormalized_image_coordinates(normalized_coor.values, 4496, 3000)\n",
    "\n",
    "# create a new column with the denormalized coordinates\n",
    "tracks[\"denorm_x\"] = denormalized_coor[:, 0]\n",
    "tracks[\"denorm_y\"] = denormalized_coor[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to store the georegistration by creating a new .tif file for every CAP image. As you can recall, .tif files save not just the pixel data but also the projection that allows it to be displayed on top of other map data. There are two parts to doing this:\n",
    "- First, we need to create an _orthorectified_ image. Simply put, this is one that is transformed such that it looks as though you are looking at it from the top down. \n",
    "- Second, we need to add *ground control points* (GCPs) to the orthorectified image. GCPs are correspondences between world coordinates and pixel coordinates.\n",
    "\n",
    "Once we add the GCPs, any mapping software can plot the image such that the GCPs are aligned with their underlying coordinates. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import gdal, osr\n",
    "try:\n",
    "    from pymap3d import enu2geodetic\n",
    "except:\n",
    "    !pip install pymap3d\n",
    "    from pymap3d import enu2geodetic\n",
    "\n",
    "import random\n",
    "from skimage import transform\n",
    "\n",
    "if not os.path.isdir(\"CAP_sample_1/geotiff/\"):\n",
    "    os.mkdir(\"CAP_sample_1/geotiff/\")\n",
    "if not os.path.isdir(\"CAP_sample_1/ortho/\"):\n",
    "    os.mkdir(\"CAP_sample_1/ortho/\")\n",
    "\n",
    "for reconst in reconstructions:\n",
    "    for shot in reconst[\"shots\"]:\n",
    "        # some housekeeping\n",
    "        shot_name = shot.split(\".\")[0]\n",
    "        img = cv2.imread(\"CAP_sample_1/images/\"+shot)\n",
    "        shape = img.shape\n",
    "        \n",
    "        # here we get the features from the image and their corresponding reconstructed features\n",
    "        reconst_ids = list(map(int, reconst[\"points\"].keys()))\n",
    "        tracks_shot = tracks[(tracks[\"image_name\"] == shot) & (tracks[\"track_id\"].isin(reconst_ids))]\n",
    "        denorm_shot = np.round(tracks_shot[[\"denorm_x\", \"denorm_y\"]].values)\n",
    "        reconst_shot = np.array([reconst[\"points\"][str(point)][\"coordinates\"] for point in tracks_shot[\"track_id\"]])\n",
    "        \n",
    "        # we're going to create an image that is distorted to fit within the world coordinates\n",
    "        # pix_shot is just the reconstructed feature coordinates offset by some amount so that\n",
    "        # all coordinates are positive.\n",
    "        offset = np.min(reconst_shot[:, :2])\n",
    "        pix_shot = reconst_shot[:, :2]-np.multiply(offset, offset<0)\n",
    "        \n",
    "        # transformation for the new orthorectified image\n",
    "        H, inliers = cv2.findHomography(denorm_shot, pix_shot)\n",
    "        \n",
    "        # filtering out points that didn't fit the transformation\n",
    "        reconst_shot = reconst_shot[inliers.ravel()==1, :]\n",
    "        denorm_shot = np.round(denorm_shot[inliers.ravel()==1, :])\n",
    "        pix_shot = np.round(pix_shot[inliers.ravel()==1, :])\n",
    "        \n",
    "        # creating the ortho image\n",
    "        shape = tuple(np.max(pix_shot, axis=0).astype(int))\n",
    "        ortho_img = cv2.warpPerspective(img, H, shape)\n",
    "        cv2.imwrite(\"CAP_sample_1/ortho/\" + shot + \"_ortho.jpg\", ortho_img)\n",
    "        \n",
    "        # here we convert all of the reconstructed points into lat/lon coordinates\n",
    "        geo_shot = np.array([enu2geodetic(reconst_shot[i, 0],reconst_shot[i, 1],reconst_shot[i, 2],latitude,longitude,altitude) for i in range(reconst_shot.shape[0])])        \n",
    "        \n",
    "        idx = random.sample(range(len(geo_shot)), 10)\n",
    "        pix_shot_sample = pix_shot[idx, :]\n",
    "        geo_shot_sample = geo_shot[idx, :]\n",
    "                \n",
    "        # creating the Ground Control Points\n",
    "        orig_fn = \"CAP_sample_1/ortho/\" + shot + \"_ortho.jpg\"\n",
    "        fn = \"CAP_sample_1/geotiff/\" + shot_name + \"_GCP.tif\"\n",
    "        \n",
    "        orig_ds = gdal.Open(orig_fn)\n",
    "        gdal.GetDriverByName('GTiff').CreateCopy(fn, orig_ds)\n",
    "        ds = gdal.Open(fn, gdal.GA_Update)\n",
    "        sr = osr.SpatialReference()\n",
    "        sr.SetWellKnownGeogCS('WGS84')\n",
    "        \n",
    "        gcps = [gdal.GCP(geo_shot_sample[i, 1], geo_shot_sample[i, 0], 0, int(pix_shot_sample[i, 0]), int(pix_shot_sample[i, 1])) for i in range(geo_shot_sample.shape[0])]\n",
    "        \n",
    "        ds.SetGCPs(gcps, sr.ExportToWkt())\n",
    "        \n",
    "        ds = None\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "In the lesson folder, there is a spreadsheet with CAP images and their coordinates taken on October 13th, 2017. \n",
    "- Use geopandas to visualize the coordinates of all the images, and overlay it with some basemap\n",
    "- Select an area of those images that looks interesting to you. Use SfM to reconstruct at least 10 images\n",
    "- For those 10 images, select at least one and go through the georegistration process. Does the georegistration process yield good alignment with the ground truth? If not, why do you think that is?\n",
    "\n",
    "I **strongly** encourage you to tackle this as a team! Feel free to divide the tasks up as you see fit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
