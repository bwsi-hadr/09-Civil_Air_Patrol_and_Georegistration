{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Civil Air Patrol and Georegistration\n",
    "\n",
    "In the previous session, we saw how a series of 2D images taken of a 3D scene can be used to recover the 3D information, by exploiting geometric constraints of the cameras. Now the question is, how do we take this technique and apply it in a disaster response scenario?\n",
    "\n",
    "We are going to look at a specific case study, using images from the Low Altitude Disaster Imagery (LADI) dataset, taken by the Civil Air Patrol (CAP). As we work with this dataset, keep in mind the two major questions from the previous lecture:\n",
    "\n",
    "- _What_ is in an image (e.g. debris, buildings, etc.)?\n",
    "- _Where_ are these things located _in 3D space_ ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Civil Air Patrol\n",
    "Civil Air Patrol (CAP) is the civilian auxiliary of the United States Air Force (USAF). The origins of CAP date back to the pre-World War II era. As the Axis powers became a growing threat to the world, civilian aviators in the United States feared that the government would shut down general aviation as a precautionary measure. These aviators thus had to prove to the federal government that civilian aviation was not only not a danger, but actually a benefit to the war effort. \n",
    "\n",
    "As a result of these efforts, two separate programs were created. One was a Civilian Pilot Training Program, intended to increase the available people that could operate an aircraft should the need to deploy additional troops arise. The second actually called for the organization of civilian aviators and opened the door to the creation of CAP. \n",
    "\n",
    "Once the United States entered WWII proper, CAP began to embark a plethora of activities, some of which are still practiced today. They continued to do cadet education programs. They also began patrolling the coasts and borders. Finally, they started in 1942 conducting search and rescue (SAR) missions. These missions were a resounding success, and one of the main components of CAP today.\n",
    "\n",
    "CAP has five congressionally mandated missions:\n",
    "\n",
    "(1) To provide an organization toâ€”\n",
    "(A) encourage and aid citizens of the United States in contributing their efforts, services, and resources in developing aviation and in maintaining air supremacy; and\n",
    "(B) encourage and develop by example the voluntary contribution of private citizens to the public welfare.\n",
    "\n",
    "(2) To provide aviation education and training especially to its senior and cadet members.\n",
    "\n",
    "(3) To encourage and foster civil aviation in local communities.\n",
    "\n",
    "(4) To provide an organization of private citizens with adequate facilities to assist in meeting local and national emergencies.\n",
    "\n",
    "(5) To assist the Department of the Air Force in fulfilling its noncombat programs and missions.\n",
    "\n",
    "source: https://www.law.cornell.edu/uscode/text/36/40302\n",
    "\n",
    "CAP's main series of missions revolve around emergency response. CAP is involved in roughly 85% of all SAR missions in the United States and its territories. After natural disasters, CAP is responsible for assessing damage in affected communities, delivering supplies, providing transportation, in addition to its usual SAR missions. \n",
    "\n",
    "https://kvia.com/health/2020/06/18/el-paso-civil-air-patrol-flying-virus-tests-to-labs-in-money-saving-effort-for-texas/\n",
    "\n",
    "https://www.southernminn.com/article_2c5739a5-826f-53bb-a658-922fb1aa1627.html\n",
    "\n",
    "Part of their emergency programming is taking aerial imagery of affected areas. This imagery is the highest resolution, most timely imagery that we have available of a post-disaster situation. Even the highest resolution satellite imagery is often either limited in their geographical coverage, not very timely or occluded by clouds. These are images taken of Puerto Rico after Hurricane Maria in 2017.\n",
    "\n",
    "<img src=\"notebook_images/A0008AP-_932ec345-75a9-4005-9879-da06ba0af37e.jpg\" width=\"500\"  />\n",
    "\n",
    "<img src=\"notebook_images/A0016-52_e71b5e09-ec3c-4ea6-8ac8-b9d1e4b714cb.jpg\" width=\"500\"  />\n",
    "\n",
    "<img src=\"notebook_images/A0016-54_f5273b60-dec4-4617-8f01-d67f16001dcb.jpg\" width=\"500\"  />\n",
    "\n",
    "CAP has taken hundreds of thousands of images of disaster-affected areas in the past decades. And yet, even though it is some of the best imagery we have access to, it is rarely if ever used in practice. _Why?_\n",
    "\n",
    "## The LADI dataset\n",
    "Part of the effort in making CAP imagery more useful is trying to make more sense of the content of the images. To that end, researchers at MIT Lincoln Laboratory released the Low Altitude Disaster Imagery (LADI) dataset. This dataset contains hundreds of thousands of CAP images that have crowdsourced labels corresponding to infrastructure, environment and damage categories. This begins to answer the first of the two questions we set out initially. We'll start working on these labels tomorrow. For now, we will solely focus on the images themselves.\n",
    "\n",
    "<img src=\"notebook_images/labels.png\" width=\"500\"  />\n",
    "\n",
    "What are some of the limitations of this dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Imagine you have acquired $200,000 to implement some improvement to the way CAP takes aerial imagery. Hurricane season starts in five months, so whatever improvements need to be implemented by then. Separate into your breakout rooms and answer the following questions:\n",
    "- What specific hurdles to using CAP images do you want to address? Identify at least two.\n",
    "- Design a proposal to address the challenges you identified above, taking into account the budget and time constraints. Improvements can be of any sort (technical, political, social, etc).\n",
    "- What are the advantages and disadvantages of implementing your proposal?\n",
    "- Identify at least three different stakeholder groups in this situation. What are their specific needs? How does your proposal address these needs? How does your proposal fall short?\n",
    "- Draw out a budget breakdown and a timeline, as well as a breakdown of which stakeholders you are prioritizing and why. Prepare to present these at 1:30pm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3D Reconstruction in a Real World Reference Frame\n",
    "\n",
    "If we want to answer the second of our two guiding questions, we must be able to make a translation between where something is in the image and its location in real world coordinates. Let's stake stock of what tools we have thus far. We spent a good amount of time discussing structure from motion as a way to reconstruct a 3D scene from 2D images. Recall the limitations of this approach:\n",
    "- There need to be more than one image in a sequence.\n",
    "- Sequential images need to have enough overlap that there are common features.\n",
    "- At least one pair of sequential images must have sufficient translation such that the problem is not ill-posed.\n",
    "- The reconstruction is given in an arbitrary reference frame up to scale. \n",
    "\n",
    "What does that last point mean? The arbitrary reference part refers to the fact that the origin and the axes are aligned with the first camera. The up to scale part means that all distances are preserved up to a factor $\\lambda$. Therefore the scene retains the general shape, but the size of the scene is not conserved. Without additional information, it is impossible to know how the reconstructed scene relates to any other reference frame, and translating the reconstruction to real world coordinates is impossible.\n",
    "\n",
    "However, recall that we do typically have at least a coarse estimate of the camera's GPS coordinates, therefore we have estimates of the distances between sequential cameras. Consider a reconstruction of just two images. Then a good estimate of $\\lambda$ is:\n",
    "\n",
    "$\\lambda = \\frac{D_{GPS}}{D_{reconstruction}}$\n",
    "\n",
    "This is slightly more complicated for more than two images. Typically, a solver will initialize the camera positions at their GPS coordinates and use bundle adjustment to correct the errors in the GPS measurements, although certainly there's more than one way to do this.\n",
    "\n",
    "Let's give this a shot and see what happens! As it so happens, OpenSfM is already equipped to handle GPS coordinates. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import open3d as o3d\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-07-14 20:35:08,977 INFO: Loading existing EXIF for image_url_pr_10_13_sample_07.jpg\n",
      "2020-07-14 20:35:08,977 INFO: Loading existing EXIF for image_url_pr_10_13_sample_12.jpg\n",
      "2020-07-14 20:35:08,977 INFO: Loading existing EXIF for image_url_pr_10_13_sample_10.jpg\n",
      "2020-07-14 20:35:08,977 INFO: Loading existing EXIF for image_url_pr_10_13_sample_08.jpg\n",
      "2020-07-14 20:35:08,977 INFO: Loading existing EXIF for image_url_pr_10_13_sample_11.jpg\n",
      "2020-07-14 20:35:08,978 INFO: Loading existing EXIF for image_url_pr_10_13_sample_13.jpg\n",
      "2020-07-14 20:35:10,447 INFO: Skip recomputing ROOT_HAHOG features for image image_url_pr_10_13_sample_07.jpg\n",
      "2020-07-14 20:35:10,447 INFO: Skip recomputing ROOT_HAHOG features for image image_url_pr_10_13_sample_12.jpg\n",
      "2020-07-14 20:35:10,447 INFO: Skip recomputing ROOT_HAHOG features for image image_url_pr_10_13_sample_10.jpg\n",
      "2020-07-14 20:35:10,448 INFO: Skip recomputing ROOT_HAHOG features for image image_url_pr_10_13_sample_08.jpg\n",
      "2020-07-14 20:35:10,448 INFO: Skip recomputing ROOT_HAHOG features for image image_url_pr_10_13_sample_11.jpg\n",
      "2020-07-14 20:35:10,448 INFO: Skip recomputing ROOT_HAHOG features for image image_url_pr_10_13_sample_13.jpg\n",
      "2020-07-14 20:35:11,926 INFO: Matching 8 image pairs\n",
      "2020-07-14 20:35:11,934 INFO: Computing pair matching with 1 processes\n",
      "2020-07-14 20:35:11,966 DEBUG: No segmentation for image_url_pr_10_13_sample_10.jpg, no features masked.\n",
      "2020-07-14 20:35:11,980 DEBUG: No segmentation for image_url_pr_10_13_sample_12.jpg, no features masked.\n",
      "2020-07-14 20:35:12,604 DEBUG: Matching image_url_pr_10_13_sample_10.jpg and image_url_pr_10_13_sample_12.jpg.  Matcher: FLANN (symmetric) T-desc: 0.635 T-robust: 0.002 T-total: 0.636 Matches: 623 Robust: 617 Success: True\n",
      "2020-07-14 20:35:12,615 DEBUG: No segmentation for image_url_pr_10_13_sample_07.jpg, no features masked.\n",
      "2020-07-14 20:35:12,936 DEBUG: Matching image_url_pr_10_13_sample_10.jpg and image_url_pr_10_13_sample_07.jpg.  Matcher: FLANN (symmetric) T-desc: 0.322 Matches: FAILED\n",
      "2020-07-14 20:35:12,946 DEBUG: No segmentation for image_url_pr_10_13_sample_08.jpg, no features masked.\n",
      "2020-07-14 20:35:13,271 DEBUG: Matching image_url_pr_10_13_sample_10.jpg and image_url_pr_10_13_sample_08.jpg.  Matcher: FLANN (symmetric) T-desc: 0.325 T-robust: 0.001 T-total: 0.326 Matches: 30 Robust: 23 Success: True\n",
      "2020-07-14 20:35:13,271 DEBUG: Image image_url_pr_10_13_sample_10.jpg matches: 2 out of 3\n",
      "2020-07-14 20:35:13,287 DEBUG: No segmentation for image_url_pr_10_13_sample_11.jpg, no features masked.\n",
      "2020-07-14 20:35:13,288 DEBUG: No segmentation for image_url_pr_10_13_sample_13.jpg, no features masked.\n",
      "2020-07-14 20:35:13,799 DEBUG: Matching image_url_pr_10_13_sample_11.jpg and image_url_pr_10_13_sample_13.jpg.  Matcher: FLANN (symmetric) T-desc: 0.510 T-robust: 0.001 T-total: 0.511 Matches: 540 Robust: 533 Success: True\n",
      "2020-07-14 20:35:13,961 DEBUG: Matching image_url_pr_10_13_sample_11.jpg and image_url_pr_10_13_sample_10.jpg.  Matcher: FLANN (symmetric) T-desc: 0.154 T-robust: 0.005 T-total: 0.159 Matches: 1800 Robust: 1791 Success: True\n",
      "2020-07-14 20:35:14,127 DEBUG: Matching image_url_pr_10_13_sample_11.jpg and image_url_pr_10_13_sample_12.jpg.  Matcher: FLANN (symmetric) T-desc: 0.159 T-robust: 0.004 T-total: 0.163 Matches: 1747 Robust: 1741 Success: True\n",
      "2020-07-14 20:35:14,127 DEBUG: Image image_url_pr_10_13_sample_11.jpg matches: 3 out of 3\n",
      "2020-07-14 20:35:14,288 DEBUG: Matching image_url_pr_10_13_sample_07.jpg and image_url_pr_10_13_sample_08.jpg.  Matcher: FLANN (symmetric) T-desc: 0.159 T-robust: 0.002 T-total: 0.160 Matches: 756 Robust: 728 Success: True\n",
      "2020-07-14 20:35:14,289 DEBUG: Image image_url_pr_10_13_sample_07.jpg matches: 1 out of 1\n",
      "2020-07-14 20:35:14,458 DEBUG: Matching image_url_pr_10_13_sample_12.jpg and image_url_pr_10_13_sample_13.jpg.  Matcher: FLANN (symmetric) T-desc: 0.163 T-robust: 0.004 T-total: 0.167 Matches: 1704 Robust: 1686 Success: True\n",
      "2020-07-14 20:35:14,459 DEBUG: Image image_url_pr_10_13_sample_12.jpg matches: 1 out of 1\n",
      "2020-07-14 20:35:14,459 DEBUG: Image image_url_pr_10_13_sample_08.jpg matches: 0 out of 0\n",
      "2020-07-14 20:35:14,459 DEBUG: Image image_url_pr_10_13_sample_13.jpg matches: 0 out of 0\n",
      "2020-07-14 20:35:14,459 INFO: Matched 8 pairs for 6 ref_images (perspective-perspective: 8) in 2.5328616830011015 seconds (0.31660776162516413 seconds/pair).\n",
      "2020-07-14 20:35:16,033 INFO: reading features\n",
      "2020-07-14 20:35:16,088 DEBUG: Merging features onto tracks\n",
      "2020-07-14 20:35:16,145 DEBUG: Good tracks: 4329\n",
      "2020-07-14 20:35:17,776 INFO: Starting incremental reconstruction\n",
      "2020-07-14 20:35:17,836 INFO: Starting reconstruction with image_url_pr_10_13_sample_10.jpg and image_url_pr_10_13_sample_11.jpg\n",
      "2020-07-14 20:35:17,864 INFO: Two-view reconstruction inliers: 1816 / 1816\n",
      "2020-07-14 20:35:18,027 INFO: Triangulated: 1476\n",
      "2020-07-14 20:35:18,099 DEBUG: Ceres Solver Report: Iterations: 3, Initial cost: 2.867015e+02, Final cost: 2.858874e+02, Termination: CONVERGENCE\n",
      "2020-07-14 20:35:18,231 DEBUG: Ceres Solver Report: Iterations: 2, Initial cost: 2.860912e+02, Final cost: 2.858800e+02, Termination: CONVERGENCE\n",
      "2020-07-14 20:35:19,274 DEBUG: Ceres Solver Report: Iterations: 97, Initial cost: 1.618729e+01, Final cost: 1.465635e+01, Termination: CONVERGENCE\n",
      "2020-07-14 20:35:19,280 INFO: Removed outliers: 0\n",
      "2020-07-14 20:35:19,281 INFO: -------------------------------------------------------\n",
      "2020-07-14 20:35:19,300 INFO: image_url_pr_10_13_sample_12.jpg resection inliers: 749 / 752\n",
      "2020-07-14 20:35:19,325 DEBUG: Ceres Solver Report: Iterations: 5, Initial cost: 1.345598e+02, Final cost: 4.372846e+01, Termination: CONVERGENCE\n",
      "2020-07-14 20:35:19,325 INFO: Adding image_url_pr_10_13_sample_12.jpg to the reconstruction\n",
      "2020-07-14 20:35:19,507 INFO: Re-triangulating\n",
      "2020-07-14 20:35:20,631 DEBUG: Ceres Solver Report: Iterations: 47, Initial cost: 9.229371e+01, Final cost: 4.998238e+01, Termination: CONVERGENCE\n",
      "2020-07-14 20:35:21,252 DEBUG: Ceres Solver Report: Iterations: 17, Initial cost: 5.896264e+01, Final cost: 5.735602e+01, Termination: CONVERGENCE\n",
      "2020-07-14 20:35:21,267 INFO: Removed outliers: 2\n",
      "2020-07-14 20:35:21,268 INFO: -------------------------------------------------------\n",
      "2020-07-14 20:35:21,282 INFO: image_url_pr_10_13_sample_13.jpg resection inliers: 877 / 878\n",
      "2020-07-14 20:35:21,311 DEBUG: Ceres Solver Report: Iterations: 5, Initial cost: 1.792389e+02, Final cost: 8.751818e+01, Termination: CONVERGENCE\n",
      "2020-07-14 20:35:21,311 INFO: Adding image_url_pr_10_13_sample_13.jpg to the reconstruction\n",
      "2020-07-14 20:35:21,420 INFO: Re-triangulating\n",
      "2020-07-14 20:35:22,687 DEBUG: Ceres Solver Report: Iterations: 34, Initial cost: 1.528225e+02, Final cost: 9.822382e+01, Termination: CONVERGENCE\n",
      "2020-07-14 20:35:23,308 DEBUG: Ceres Solver Report: Iterations: 9, Initial cost: 1.041841e+02, Final cost: 1.026677e+02, Termination: CONVERGENCE\n",
      "2020-07-14 20:35:23,327 INFO: Removed outliers: 2\n",
      "2020-07-14 20:35:23,328 INFO: -------------------------------------------------------\n",
      "2020-07-14 20:35:23,333 INFO: image_url_pr_10_13_sample_08.jpg resection inliers: 13 / 16\n",
      "2020-07-14 20:35:23,334 DEBUG: Ceres Solver Report: Iterations: 8, Initial cost: 9.952333e+01, Final cost: 3.094382e+01, Termination: CONVERGENCE\n",
      "2020-07-14 20:35:23,334 INFO: Adding image_url_pr_10_13_sample_08.jpg to the reconstruction\n",
      "2020-07-14 20:35:23,345 DEBUG: Local bundle sets: interior 1  boundary 4  other 0\n",
      "2020-07-14 20:35:23,348 DEBUG: Ceres Solver Report: Iterations: 5, Initial cost: 3.167329e+01, Final cost: 1.151650e+01, Termination: CONVERGENCE\n",
      "2020-07-14 20:35:23,348 INFO: Removed outliers: 0\n",
      "2020-07-14 20:35:23,349 INFO: -------------------------------------------------------\n",
      "2020-07-14 20:35:23,355 INFO: image_url_pr_10_13_sample_07.jpg resection inliers: 11 / 13\n",
      "2020-07-14 20:35:23,356 DEBUG: Ceres Solver Report: Iterations: 6, Initial cost: 1.449346e+01, Final cost: 1.284201e+01, Termination: CONVERGENCE\n",
      "2020-07-14 20:35:23,357 INFO: Adding image_url_pr_10_13_sample_07.jpg to the reconstruction\n",
      "2020-07-14 20:35:23,420 INFO: Re-triangulating\n",
      "2020-07-14 20:35:26,144 DEBUG: Ceres Solver Report: Iterations: 71, Initial cost: 5.240593e+02, Final cost: 1.126203e+02, Termination: CONVERGENCE\n",
      "2020-07-14 20:35:26,990 DEBUG: Ceres Solver Report: Iterations: 13, Initial cost: 1.220495e+02, Final cost: 1.195928e+02, Termination: CONVERGENCE\n",
      "2020-07-14 20:35:27,012 INFO: Removed outliers: 2\n",
      "2020-07-14 20:35:27,013 INFO: -------------------------------------------------------\n",
      "2020-07-14 20:35:28,276 DEBUG: Ceres Solver Report: Iterations: 26, Initial cost: 1.207186e+02, Final cost: 1.159228e+02, Termination: CONVERGENCE\n",
      "2020-07-14 20:35:28,299 INFO: Removed outliers: 0\n",
      "2020-07-14 20:35:28,343 INFO: {'points_count': 4326, 'cameras_count': 6, 'observations_count': 10480, 'average_track_length': 2.422561257512714, 'average_track_length_notwo': 3.3372348207754206}\n",
      "2020-07-14 20:35:28,343 INFO: Reconstruction 0: 6 images, 4326 points\n",
      "2020-07-14 20:35:28,343 INFO: 1 partial reconstructions in total.\n"
     ]
    }
   ],
   "source": [
    "# Take initial guess of intrinsic parameters through metadata\n",
    "!opensfm extract_metadata CAP_sample_1\n",
    "\n",
    "# Detect features points \n",
    "!opensfm detect_features CAP_sample_1\n",
    "\n",
    "# Match feature points across images\n",
    "!opensfm match_features CAP_sample_1\n",
    "\n",
    "# This creates \"tracks\" for the features. That is to say, if a feature in image 1 is matched with one in image 2,\n",
    "# and in turn that one is matched with one in image 3, then it links the matches between 1 and 3. \n",
    "!opensfm create_tracks CAP_sample_1\n",
    "\n",
    "# Calculates the essential matrix, the camera pose and the reconstructed feature points\n",
    "!opensfm reconstruct CAP_sample_1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding the --all command to include all partial reconstructions\n",
    "!opensfm export_ply --all CAP_sample_1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "621a3d6d00bb4c7fafdf8c7c9473eb32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "JVisualizer with 1 geometries"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import open3d as o3d\n",
    "from open3d import JVisualizer\n",
    "\n",
    "# it turns out that we have two partial reconstructions from the reconstruct command\n",
    "# open3d actually has a very convenient way of combining point clouds, just by using the + operator\n",
    "pcd = o3d.io.read_point_cloud(\"CAP_sample_1/reconstruction_files/reconstruction_0.ply\")\n",
    "pcd += o3d.io.read_point_cloud(\"CAP_sample_1/reconstruction_files/reconstruction_1.ply\")\n",
    "visualizer = JVisualizer()\n",
    "visualizer.add_geometry(pcd)\n",
    "visualizer.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what are we seeing? We see two collections of points, both mostly coplanar internally (which we expect, given that this is a mostly planar scene), but the two sets are not aligned with each other! **What's going on?** \n",
    "\n",
    "### Exercise\n",
    "Let's make a critical assumption: all of the image coordinates (the GPS coordinates of the camera as it takes an image) all lie on a plane (in the mathematical sense). Answer the following questions:\n",
    "- How many points are needed to specify a (mathematical) plane?\n",
    "- In addition to the number of points, what other requirement do those points need?\n",
    "- Look at the visualization above. Do the camera points fulfill that requirement?\n",
    "- One way to resolve the ambiguity is to determine what direction is \"up\" (i.e. pointing away from the center of the Earth). Propose a solution to determine the up-vector. You can either assume the same setup that we currently have or propose new sensors/other setups.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>CLICK HERE TO SEE THE PROPOSED SOLUTION</summary>\n",
    "    We're going to make a fair (but limited) assumption that the ground is mostly flat. It turns out we can fit a plane through the reconstructed ground points and find a direction perpendicular to the plane (called the plane normal). If the ground is flat, then the normal should be close enough to the up direction. Note that this assumption does not hold for an area with a lot of inclination. In practice, we would most likely augment this with a Digital Elevation Model (DEM)\n",
    "\n",
    "</details>\n",
    "\n",
    "<img src=\"notebook_images/plane_normal.png\" width=\"500\"  />\n",
    "\n",
    "To implement the proposed solution, we can go to the CAP_sample_1/config.yaml file and modify \"align_orientation_prior\" from \"horizontal\" to \"plane_based\". Afterwards, we run the previous commands as usual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: opensfm: command not found\n"
     ]
    }
   ],
   "source": [
    "# This creates \"tracks\" for the features. That is to say, if a feature in image 1 is matched with one in image 2,\n",
    "# and in turn that one is matched with one in image 3, then it links the matches between 1 and 3. \n",
    "!opensfm create_tracks CAP_sample_1\n",
    "\n",
    "# Calculates the essential matrix, the camera pose and the reconstructed feature points\n",
    "!opensfm reconstruct CAP_sample_1\n",
    "\n",
    "# adding the --all command to include all partial reconstructions\n",
    "!opensfm export_ply --all CAP_sample_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Georegistration\n",
    "\n",
    "The process of assigning GPS coordinates to individual pixels is called _georegistration_ or _georeferencing_. This requires us to perform a final transformation from pixel coordinates *per each image* to the 3D reconstructed coordinates. Before doing so, it is worthwhile talking a bit about what exactly our 3D coordinate system is. \n",
    "\n",
    "You might recall that not all coordinate referece systems lend themselves well to geometric transformations. Specifically, we want our 3D coordinate system to be Cartesian (i.e. three orthogonal, right-handed axes). OpenSfM performs its reconstructions in what is known as a *local tangent plane coordinate system* called *local east, north, up (ENU) coordinates*. The way this works is, you select an origin somewhere in the world (in our case, it is saved in the reference_lla.json file), and you align your axes such that the x-axis is parallel to latitudes and increasing Eastward, the y-axis is parallel to meridians and increasing Northward, and the z-axis is pointing away from the center of the Earth. The image below shows how this works:\n",
    "\n",
    "<img src=\"notebook_images/enu.png\" width=\"500\"  />\n",
    "\n",
    "In order to convert from ENU coordinates to geodetic coordinates (i.e. latitude, longitude, altitude), you need to know the origin. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Origin of our reconstruction, as given by the reference_lla.json (made from the reconstruction)\n",
    "with open(\"CAP_sample_1/reference_lla.json\", \"r\") as f:\n",
    "    reference_lla = json.load(f)\n",
    "    latitude=reference_lla[\"latitude\"]\n",
    "    longitude=reference_lla[\"longitude\"]\n",
    "    altitude=reference_lla[\"altitude\"]\n",
    "\n",
    "# This is the json file that contains the reconstructed feature points\n",
    "with open(\"CAP_sample_1/reconstruction.json\", \"r\") as f:\n",
    "    reconstructions = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a bit of work we need to go through to finalize the georegistration. First, we need to match the reconstructed features with the features on an image the tracks.csv file and the reconstruction.json can help us do that. The columns of tracks are as follows: image name, track ID (ID of the reconstructed point), feature ID (ID of the feature within the image), the *normalized* image coordinates x and y, the normalization factor s, and the color of the feature RGB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from opensfm.features import denormalized_image_coordinates\n",
    "\n",
    "# reading the csv\n",
    "tracks = pd.read_csv(\"CAP_sample_1/tracks.csv\", sep=\"\\t\", skiprows=1, names=[\"image_name\", \"track_id\", \"feature_id\", \"x\", \"y\", \"s\", \"R\", \"G\", \"B\"])\n",
    "\n",
    "# we need to denormalize the coordinates to turn them into regular pixel coordinates\n",
    "normalized_coor = tracks[[\"x\", \"y\", \"s\"]]\n",
    "denormalized_coor = denormalized_image_coordinates(normalized_coor.values, 4496, 3000)\n",
    "\n",
    "# create a new column with the denormalized coordinates\n",
    "tracks[\"denorm_x\"] = denormalized_coor[:, 0]\n",
    "tracks[\"denorm_y\"] = denormalized_coor[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to store the georegistration by creating a new .tif file for every CAP image. As you can recall, .tif files save not just the pixel data but also the projection that allows it to be displayed on top of other map data. There are two parts to doing this:\n",
    "- First, we need to create an _orthorectified_ image. Simply put, this is one that is transformed such that it looks as though you are looking at it from the top down. \n",
    "- Second, we need to add *ground control points* (GCPs) to the orthorectified image. GCPs are correspondences between world coordinates and pixel coordinates.\n",
    "\n",
    "Once we add the GCPs, any mapping software can plot the image such that the GCPs are aligned with their underlying coordinates. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import gdal, osr\n",
    "from pymap3d import enu2geodetic\n",
    "import random\n",
    "from skimage import transform\n",
    "\n",
    "if not os.path.isdir(\"CAP_sample_1/geotiff/\"):\n",
    "    os.mkdir(\"CAP_sample_1/geotiff/\")\n",
    "if not os.path.isdir(\"CAP_sample_1/ortho/\"):\n",
    "    os.mkdir(\"CAP_sample_1/ortho/\")\n",
    "\n",
    "for reconst in reconstructions:\n",
    "    for shot in reconst[\"shots\"]:\n",
    "        # some housekeeping\n",
    "        shot_name = shot.split(\".\")[0]\n",
    "        img = cv2.imread(\"CAP_sample_1/images/\"+shot)\n",
    "        shape = img.shape\n",
    "        \n",
    "        # here we get the features from the image and their corresponding reconstructed features\n",
    "        reconst_ids = list(map(int, reconst[\"points\"].keys()))\n",
    "        tracks_shot = tracks[(tracks[\"image_name\"] == shot) & (tracks[\"track_id\"].isin(reconst_ids))]\n",
    "        denorm_shot = np.round(tracks_shot[[\"denorm_x\", \"denorm_y\"]].values)\n",
    "        reconst_shot = np.array([reconst[\"points\"][str(point)][\"coordinates\"] for point in tracks_shot[\"track_id\"]])\n",
    "        \n",
    "        # we're going to create an image that is distorted to fit within the world coordinates\n",
    "        # pix_shot is just the reconstructed feature coordinates offset by some amount so that\n",
    "        # all coordinates are positive.\n",
    "        offset = np.min(reconst_shot[:, :2])\n",
    "        pix_shot = reconst_shot[:, :2]-np.multiply(offset, offset<0)\n",
    "        \n",
    "        # transformation for the new orthorectified image\n",
    "        H, inliers = cv2.findHomography(denorm_shot, pix_shot)\n",
    "        \n",
    "        # filtering out points that didn't fit the transformation\n",
    "        reconst_shot = reconst_shot[inliers.ravel()==1, :]\n",
    "        denorm_shot = np.round(denorm_shot[inliers.ravel()==1, :])\n",
    "        pix_shot = np.round(pix_shot[inliers.ravel()==1, :])\n",
    "        \n",
    "        # creating the ortho image\n",
    "        shape = tuple(np.max(pix_shot, axis=0).astype(int))\n",
    "        ortho_img = cv2.warpPerspective(img, H, shape)\n",
    "        cv2.imwrite(\"CAP_sample_1/ortho/\" + shot + \"_ortho.jpg\", ortho_img)\n",
    "        \n",
    "        # here we convert all of the reconstructed points into lat/lon coordinates\n",
    "        geo_shot = np.array([enu2geodetic(reconst_shot[i, 0],reconst_shot[i, 1],reconst_shot[i, 2],latitude,longitude,altitude) for i in range(reconst_shot.shape[0])])        \n",
    "        \n",
    "        idx = random.sample(range(len(geo_shot)), 10)\n",
    "        pix_shot_sample = pix_shot[idx, :]\n",
    "        geo_shot_sample = geo_shot[idx, :]\n",
    "                \n",
    "        # creating the Ground Control Points\n",
    "        orig_fn = \"CAP_sample_1/ortho/\" + shot + \"_ortho.jpg\"\n",
    "        fn = \"CAP_sample_1/geotiff/\" + shot_name + \"_GCP.tif\"\n",
    "        \n",
    "        orig_ds = gdal.Open(orig_fn)\n",
    "        gdal.GetDriverByName('GTiff').CreateCopy(fn, orig_ds)\n",
    "        ds = gdal.Open(fn, gdal.GA_Update)\n",
    "        sr = osr.SpatialReference()\n",
    "        sr.SetWellKnownGeogCS('WGS84')\n",
    "        \n",
    "        gcps = [gdal.GCP(geo_shot_sample[i, 1], geo_shot_sample[i, 0], 0, int(pix_shot_sample[i, 0]), int(pix_shot_sample[i, 1])) for i in range(geo_shot_sample.shape[0])]\n",
    "        \n",
    "        ds.SetGCPs(gcps, sr.ExportToWkt())\n",
    "        \n",
    "        ds = None\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "In the lesson folder, there is a spreadsheet with CAP images and their coordinates taken on October 13th, 2017. \n",
    "- Use geopandas to visualize the coordinates of all the images, and overlay it with some basemap\n",
    "- Select an area of those images that looks interesting to you. Use SfM to reconstruct at least 10 images\n",
    "- For those 10 images, select at least one and go through the georegistration process. Does the georegistration process yield good alignment with the ground truth? If not, why do you think that is?\n",
    "\n",
    "I **strongly** encourage you to tackle this as a team! Feel free to divide the tasks up as you see fit. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
